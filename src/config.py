PRETRAIN_MODEL_NAME = "openchat/openchat-3.5-0106"  # it is too clever, Hard to improve further with LoRA unless data is very domain-specific or changes style drastically.
# PRETRAIN_MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0" # Very lightweight, fast to fine-tune <-- but too dumb
# PRETRAIN_MODEL_NAME = "lmsys/vicuna-7b-v1.5" # Lightweight, good for experimentation, open weights <-- not follow instruction well, like json output, make decision
# PRETRAIN_MODEL_NAME = "openchat/openchat_v3.1" 